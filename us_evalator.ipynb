{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18e442",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c95b61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_type = \"openai\"\n",
    "openai.api_version = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811bd3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "criteria_config = {\n",
    "    \"well-formed\": lambda story, ac: story,\n",
    "    \"atomic\": lambda story, ac: story,\n",
    "    \"minimal\": lambda story, ac: story,\n",
    "    \"problem-oriented\": lambda story, ac: story,\n",
    "    \"internal-consistency\": lambda story, ac: story,\n",
    "    # 默认情况：story + ac\n",
    "    \"_default\": lambda story, ac: f\"{story}\\n\\n{ac}\"\n",
    "}\n",
    "\n",
    "def build_user_content(criteria: str, story: str, ac: str) -> str:\n",
    "    func = criteria_config.get(criteria, criteria_config[\"_default\"])\n",
    "    return func(story, ac)\n",
    "\n",
    "# 模型配置\n",
    "models = [\n",
    "    #{\"name\": \"deepseek\", \"model\": \"deepseek-v3\"},\n",
    "    #{\"name\": \"qwen\", \"model\": \"qwen2.5-72b-instruct\"}\n",
    "    {\"name\":\"qwen-plus\", \"model\": \"qwen-plus\"}\n",
    "]\n",
    "\n",
    "# well-formed, minimal, atomic, unambiguous, complete, conceptually-sound, problem-oriented\n",
    "# 配置准则和Prompt类型\n",
    "criterias = ['conceptually-sound']\n",
    "sc = 3  # 每个模型评估次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cbf6b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_from_file(path: str) -> str:\n",
    "    try:\n",
    "        # 读取模板文件\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 错误：文件 {path} 未找到\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 错误读取文件：{e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_json_text(text: str) -> str:\n",
    "    return re.sub(r\"^```(json)?|```$\", \"\", text.strip(), flags=re.IGNORECASE)\n",
    "\n",
    "def evaluate_user_story(model_name, system_prompt, user_content : str, max_retries: int = 2, retry_interval: int = 2) -> dict:\n",
    "    \"\"\"调用 LLM 评估用户故事，支持自动重试\"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"{user_content}\"}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            cleaned = clean_json_text(content)\n",
    "\n",
    "            # 尝试解析为 JSON\n",
    "            parsed = json.loads(cleaned)\n",
    "            return parsed\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️ 第 {attempt} 次解析失败：返回内容非 JSON，有效格式问题。\")\n",
    "            print(\"响应内容：\", content)\n",
    "            if attempt == max_retries:\n",
    "                return {\"error\": \"Invalid JSON\", \"raw\": content}\n",
    "            time.sleep(retry_interval)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 第 {attempt} 次调用出错：{e}\")\n",
    "            if attempt == max_retries:\n",
    "                return {\"error\": str(e)}\n",
    "            time.sleep(retry_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f85346",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "template = load_from_file(\"./prompt/template\")\n",
    "prompt_template = Template(template)\n",
    "stories_file_path = './data/us/stories.xlsx'\n",
    "prompt_type = \"fewshot-exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8099b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === 主循环 ===\n",
    "for criteria in criterias:\n",
    "    prompt_file = f\"{criteria}\"\n",
    "    quality_criteria = load_from_file(f\"./prompt/{criteria}/{prompt_type}\")\n",
    "    prompt = prompt_template.substitute(quality_criteria=quality_criteria)\n",
    "    if not prompt:\n",
    "        print(\"❌ Prompt 加载失败，退出程序\")\n",
    "        continue\n",
    "\n",
    "    criteria_col = criteria.capitalize()\n",
    "    us_df = pd.read_excel(stories_file_path, usecols=[\"Issue key\", \"story\", \"ac\", \"bg\", criteria_col])\n",
    "    eval_keys = pd.read_excel(f\"data/us1/{criteria}.xlsx\", usecols=[\"Issue key\"])\n",
    "    df_eval = eval_keys.merge(us_df, on=\"Issue key\", how=\"left\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for _, row in tqdm(df_eval.iterrows(), total=len(df_eval), desc=f\"Evaluating [{criteria}]\"):\n",
    "        issue_key = row[\"Issue key\"]\n",
    "        story = row[\"story\"]\n",
    "        ac = row[\"ac\"]\n",
    "        bg = row[\"bg\"]\n",
    "        expert = row[criteria_col]\n",
    "        result_row = {\"Issue key\": issue_key, \"BG\": bg, \"Story\": story, \"AC\": ac, \"Expert\": expert}\n",
    "\n",
    "        for model_cfg in models:\n",
    "            model_name = model_cfg[\"model\"]\n",
    "            model_short = model_cfg[\"name\"]\n",
    "\n",
    "            for t in range(1, sc + 1):\n",
    "                agent_score = -1\n",
    "                try:\n",
    "                    user_content = build_user_content(criteria, story, ac)\n",
    "                    result = evaluate_user_story(model_name, prompt, user_content, 3, 1)\n",
    "                    time.sleep(1)\n",
    "                    col_prefix = f\"{model_short}_{t}\"\n",
    "                    col_result = f\"{model_short}_result_{t}\"\n",
    "\n",
    "                    if isinstance(result, dict):\n",
    "                        val = result.get(f\"violation\")\n",
    "                        if val is not None:\n",
    "                            agent_score = 0 if val else 1\n",
    "\n",
    "                    result_row[col_prefix] = agent_score\n",
    "                    result_row[col_result] = json.dumps(result, indent=2, ensure_ascii=False)\n",
    "\n",
    "                except Exception as e:\n",
    "                    result_row[f\"{model_short}_{t}\"] = -1\n",
    "                    result_row[f\"{model_short}_result_{t}\"] = json.dumps({\"error\": str(e)})\n",
    "\n",
    "        all_results.append(result_row)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    out_df = pd.DataFrame(all_results)\n",
    "    output_path = f\"evaluation/{model_short}-{criteria}-{current_time}.xlsx\"\n",
    "    out_df.to_excel(output_path, index=False)\n",
    "    print(f\"✅ 保存至：{output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
