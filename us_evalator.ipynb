{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18e442",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c95b61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_type = \"openai\"\n",
    "openai.api_version = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811bd3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stories_file_path = \"data/us/stories.xlsx\"\n",
    "\n",
    "# 模型配置\n",
    "models = [\n",
    "    #{\"name\": \"deepseek\", \"model\": \"deepseek-v3\"},\n",
    "    #{\"name\": \"qwen\", \"model\": \"qwen2.5-72b-instruct\"}\n",
    "    {\"name\":\"qwen2.5-32b\", \"model\": \"qwen2.5-32b-instruct\"}\n",
    "]\n",
    "\n",
    "# well-formed, minimal, atomic, unambiguous, complete, conceptually-sound, problem-oriented\n",
    "# 配置准则和Prompt类型\n",
    "criterias = ['well-formed']\n",
    "sc = 3  # 每个模型评估次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeda299",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return text\n",
    "    # 去掉首尾的```代码块包裹\n",
    "    text = text.strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        text = text.split(\"\\n\", 1)[1] if \"\\n\" in text else text.replace(\"```\", \"\")\n",
    "    if text.endswith(\"```\"):\n",
    "        text = text[:-3]\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd8ccb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_prompt_from_file(prompt_criteria: str) -> str:\n",
    "    try:\n",
    "        with open(f\"template/{prompt_criteria}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取 prompt 文件失败: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_json_text(text: str) -> str:\n",
    "    return re.sub(r\"^```(json)?|```$\", \"\", text.strip(), flags=re.IGNORECASE)\n",
    "\n",
    "def evaluate_user_story(model_name, system_prompt, story, ac, bg: str, max_retries: int = 2, retry_interval: int = 2) -> dict:\n",
    "    \"\"\"调用 LLM 评估用户故事，支持自动重试\"\"\"\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"{story}\"}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            cleaned = clean_json_text(content)\n",
    "\n",
    "            # 尝试解析为 JSON\n",
    "            parsed = json.loads(cleaned)\n",
    "            return parsed\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️ 第 {attempt} 次解析失败：返回内容非 JSON，有效格式问题。\")\n",
    "            print(\"响应内容：\", content)\n",
    "            if attempt == max_retries:\n",
    "                return {\"error\": \"Invalid JSON\", \"raw\": content}\n",
    "            time.sleep(retry_interval)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 第 {attempt} 次调用出错：{e}\")\n",
    "            if attempt == max_retries:\n",
    "                return {\"error\": str(e)}\n",
    "            time.sleep(retry_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf465c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === 主循环 ===\n",
    "for criteria in criterias:\n",
    "    prompt_file = f\"{criteria}\"\n",
    "    prompt = load_prompt_from_file(prompt_file)\n",
    "    if not prompt:\n",
    "        print(f\"❌ Prompt `{prompt_file}` 加载失败，跳过\")\n",
    "        continue\n",
    "\n",
    "    criteria_col = criteria.capitalize()\n",
    "    us_df = pd.read_excel(stories_file_path, usecols=[\"Issue key\", \"story\", \"ac\", \"bg\", criteria_col])\n",
    "    eval_keys = pd.read_excel(f\"data/us1/{criteria}.xlsx\", usecols=[\"Issue key\"])\n",
    "    df_eval = eval_keys.merge(us_df, on=\"Issue key\", how=\"left\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for _, row in tqdm(df_eval.iterrows(), total=len(df_eval), desc=f\"Evaluating [{criteria}]\"):\n",
    "        issue_key = row[\"Issue key\"]\n",
    "        story = row[\"story\"]\n",
    "        ac = clean_text(row[\"ac\"])\n",
    "        bg = row[\"bg\"]\n",
    "        expert = row[criteria_col]\n",
    "        result_row = {\"Issue key\": issue_key, \"BG\": bg, \"Story\": story, \"AC\": ac, \"Expert\": expert}\n",
    "\n",
    "        for model_cfg in models:\n",
    "            model_name = model_cfg[\"model\"]\n",
    "            model_short = model_cfg[\"name\"]\n",
    "\n",
    "            for t in range(1, sc + 1):\n",
    "                agent_score = -1\n",
    "                try:\n",
    "                    result = evaluate_user_story(model_name, prompt, story, ac, bg, 3, 1)\n",
    "                    time.sleep(1)\n",
    "                    col_prefix = f\"{model_short}_{t}\"\n",
    "                    col_result = f\"{model_short}_result_{t}\"\n",
    "\n",
    "                    if isinstance(result, dict):\n",
    "                        val = result.get(f\"violation\")\n",
    "                        if val is not None:\n",
    "                            agent_score = 0 if val else 1\n",
    "\n",
    "                    result_row[col_prefix] = agent_score\n",
    "                    result_row[col_result] = json.dumps(result, indent=2, ensure_ascii=False)\n",
    "\n",
    "                except Exception as e:\n",
    "                    result_row[f\"{model_short}_{t}\"] = -1\n",
    "                    result_row[f\"{model_short}_result_{t}\"] = json.dumps({\"error\": str(e)})\n",
    "\n",
    "        all_results.append(result_row)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    out_df = pd.DataFrame(all_results)\n",
    "    output_path = f\"evaluation/{model_short}-{criteria}-{current_time}.xlsx\"\n",
    "    out_df.to_excel(output_path, index=False)\n",
    "    print(f\"✅ 保存至：{output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
