{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95476fb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edbe93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_type = \"openai\"\n",
    "openai.api_version = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da12212",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"qwen2.5-72b\": {\n",
    "        \"model_id\" : \"qwen2.5-72b-instruct\"\n",
    "    },\n",
    "    \"qwen2.5-32b\": {\n",
    "        \"model_id\": \"qwen2.5-32b-instruct\"\n",
    "    },\n",
    "    \"qwen2.5-14b\": {\n",
    "        \"model_id\": \"qwen2.5-14b-instruct\"\n",
    "    },\n",
    "    \"qwen2.5-1.5b\": {\n",
    "        \"model_id\": \"qwen2.5-1.5b-instruct\"\n",
    "    },\n",
    "    \"deepseekV3\": {\n",
    "        \"model_id\" : \"deepseek-v3\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_model_config(model_name: str):\n",
    "    if model_name not in MODEL_CONFIG:\n",
    "        raise ValueError(f\"Unknown Model: {model_name}\")\n",
    "    return MODEL_CONFIG[model_name]\n",
    "\n",
    "model_list = [\"qwen2.5-72b\", \"qwen2.5-32b\", \"qwen2.5-14b\", \"qwen2.5-1.5b\", \"deepseekV3\"]\n",
    "\n",
    "model_name = model_list[3]\n",
    "model = get_model_config(model_name)[\"model_id\"]\n",
    "\n",
    "prompt_type = \"zeroshot\"\n",
    "\n",
    "# well-formed\n",
    "# atomic\n",
    "# minimal\n",
    "criteria = \"well-formed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b57ee6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_prompt_from_file(prompt_criteria: str) -> str:\n",
    "    try:\n",
    "        # 读取模板文件\n",
    "        with open(f\"template/{prompt_criteria}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 错误：文件 template/{prompt_criteria}.txt 未找到\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 错误读取模板文件：{e}\")\n",
    "        return \"\"\n",
    "    \n",
    "def clean_json_text(text: str) -> str:\n",
    "    # 移除 markdown 代码块标记\n",
    "    return re.sub(r\"^```(json)?|```$\", \"\", text.strip(), flags=re.IGNORECASE)\n",
    "\n",
    "def evaluate_user_story(system_prompt, user_story: str) -> dict:\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_story}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        #print(\"🧾 模型原始输出：\", content)\n",
    "        cleaned = clean_json_text(content)\n",
    "\n",
    "        # 尝试解析为 JSON\n",
    "        parsed = json.loads(cleaned)\n",
    "        return parsed\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ 模型返回内容不是有效的 JSON：\")\n",
    "        print(content)\n",
    "        print(\"错误信息：\", e)\n",
    "        return {\"error\": \"Invalid JSON\", \"raw\": content}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 评估出错：{e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97f01d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt_file = f\"{criteria}_{prompt_type}\"\n",
    "prompt = load_prompt_from_file(prompt_file)\n",
    "if not prompt:\n",
    "    print(\"❌ Prompt 加载失败，退出程序\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab24f8f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === 读取全量用户故事数据 ===\n",
    "criteria_col = criteria.capitalize()\n",
    "us_df = pd.read_excel(\"data/us/stories.xlsx\", usecols=[\"Issue key\", \"story\", \"ac\", \"bg\", criteria_col])\n",
    "\n",
    "# === 步骤 2：读取需要评估的 issue_key 列表 ===\n",
    "atomic_df = pd.read_excel(f\"data/us1/{criteria}.xlsx\", usecols=[\"Issue key\"])\n",
    "\n",
    "# 将需要评估的 Issue key 与全量数据匹配\n",
    "df_eval = atomic_df.merge(us_df, on=\"Issue key\", how=\"left\")\n",
    "\n",
    "# 检查是否有丢失匹配项\n",
    "missing = df_eval[df_eval[\"story\"].isnull()]\n",
    "if not missing.empty:\n",
    "    print(\"⚠️ 以下 Issue key 未在 stories.xlsx 中找到：\")\n",
    "    print(missing[\"Issue key\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3d5e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for _, row in tqdm(df_eval.iterrows(), total=len(df_eval), desc=\"Evaluating user stories\"):\n",
    "    issue_key = row[\"Issue key\"]\n",
    "    description = row[\"story\"]\n",
    "    expert = row[criteria_col]\n",
    "\n",
    "    if pd.isna(description):\n",
    "        result = {\"error\": \"No description found\"}\n",
    "    else:\n",
    "        result = evaluate_user_story(prompt, description)\n",
    "        time.sleep(1)  # 每次调用后暂停 1 秒\n",
    "        agent = -1 # （默认为 -1 表示解析失败）\n",
    "        if isinstance(result, dict):\n",
    "            v = result.get(f\"violation\")\n",
    "            if v is not None:\n",
    "                agent = 0 if v else 1\n",
    "\n",
    "    results.append({\n",
    "        \"Issue key\": issue_key,\n",
    "        \"Expert\": expert,\n",
    "        \"Agent\": agent,\n",
    "        \"Result\": json.dumps(result, indent=2, ensure_ascii=False)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f4078",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 生成当前时间字符串（到分钟）\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "output_file = f\"output/{criteria}-{model_name}-{prompt_type}-{current_time}.xlsx\"\n",
    "pd.DataFrame(results).to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✅ 结果已保存至：{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fcc36e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === 指标计算 ===\n",
    "def compute_metrics(df):\n",
    "    df = df[(df[\"Expert\"].isin([0, 1])) & (df[\"Agent\"].isin([0, 1]))]\n",
    "\n",
    "    TP = ((df[\"Expert\"] == 1) & (df[\"Agent\"] == 1)).sum()\n",
    "    TN = ((df[\"Expert\"] == 0) & (df[\"Agent\"] == 0)).sum()\n",
    "    FP = ((df[\"Expert\"] == 0) & (df[\"Agent\"] == 1)).sum()\n",
    "    FN = ((df[\"Expert\"] == 1) & (df[\"Agent\"] == 0)).sum()\n",
    "\n",
    "    total = TP + TN + FP + FN\n",
    "    accuracy = (TP + TN) / total if total else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    print(\"\\n📊 评估指标：\")\n",
    "    print(f\"TP (True Positive): {TP}\")\n",
    "    print(f\"FP (False Positive): {FP}\")\n",
    "    print(f\"TN (True Negative): {TN}\")\n",
    "    print(f\"FN (False Negative): {FN}\")\n",
    "    print(f\"Accuracy 准确率: {accuracy:.4f}\")\n",
    "    print(f\"Precision 精确率: {precision:.4f}\")\n",
    "    print(f\"Recall 召回率: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# === 计算Kappa系数\n",
    "def compute_cohen_kappa(results_df):\n",
    "    \"\"\"根据 DataFrame 中的 Expert 与 Agent 列，计算 Cohen's Kappa 系数\"\"\"\n",
    "    if \"Expert\" not in results_df.columns or \"Agent\" not in results_df.columns:\n",
    "        print(\"❌ 缺少 Expert 或 Agent 列，无法计算 Kappa\")\n",
    "        return\n",
    "    \n",
    "    # 移除无效（-1）的 agent 值\n",
    "    filtered = results_df[results_df[\"Agent\"] != -1]\n",
    "\n",
    "    if filtered.empty:\n",
    "        print(\"⚠️ 无有效模型评估结果，跳过 Kappa 计算\")\n",
    "        return\n",
    "\n",
    "    y_true = filtered[\"Expert\"].astype(int)\n",
    "    y_pred = filtered[\"Agent\"].astype(int)\n",
    "\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    print(f\"📊 Cohen's Kappa 系数：{kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69e18e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(results)\n",
    "compute_cohen_kappa(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976ac97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "compute_metrics(df_result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
