{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95476fb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edbe93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_type = \"openai\"\n",
    "openai.api_version = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da12212",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"qwen2.5-72b\": {\n",
    "        \"model_id\" : \"qwen2.5-72b-instruct\"\n",
    "    },\n",
    "    \"qwen2.5-32b\": {\n",
    "        \"model_id\": \"qwen2.5-32b-instruct\"\n",
    "    },\n",
    "    \"qwen2.5-14b\": {\n",
    "        \"model_id\": \"qwen2.5-14b-instruct\"\n",
    "    },\n",
    "    \"qwen2.5-1.5b\": {\n",
    "        \"model_id\": \"qwen2.5-1.5b-instruct\"\n",
    "    },\n",
    "    \"deepseekV3\": {\n",
    "        \"model_id\" : \"deepseek-v3\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_model_config(model_name: str):\n",
    "    if model_name not in MODEL_CONFIG:\n",
    "        raise ValueError(f\"Unknown Model: {model_name}\")\n",
    "    return MODEL_CONFIG[model_name]\n",
    "\n",
    "model_list = [\"qwen2.5-72b\", \"qwen2.5-32b\", \"qwen2.5-14b\", \"qwen2.5-1.5b\", \"deepseekV3\"]\n",
    "\n",
    "model_name = model_list[3]\n",
    "model = get_model_config(model_name)[\"model_id\"]\n",
    "\n",
    "prompt_type = \"zeroshot\"\n",
    "\n",
    "# well-formed\n",
    "# atomic\n",
    "# minimal\n",
    "criteria = \"well-formed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b57ee6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_prompt_from_file(prompt_criteria: str) -> str:\n",
    "    try:\n",
    "        # è¯»å–æ¨¡æ¿æ–‡ä»¶\n",
    "        with open(f\"template/{prompt_criteria}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šæ–‡ä»¶ template/{prompt_criteria}.txt æœªæ‰¾åˆ°\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é”™è¯¯è¯»å–æ¨¡æ¿æ–‡ä»¶ï¼š{e}\")\n",
    "        return \"\"\n",
    "    \n",
    "def clean_json_text(text: str) -> str:\n",
    "    # ç§»é™¤ markdown ä»£ç å—æ ‡è®°\n",
    "    return re.sub(r\"^```(json)?|```$\", \"\", text.strip(), flags=re.IGNORECASE)\n",
    "\n",
    "def evaluate_user_story(system_prompt, user_story: str) -> dict:\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_story}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        #print(\"ğŸ§¾ æ¨¡å‹åŸå§‹è¾“å‡ºï¼š\", content)\n",
    "        cleaned = clean_json_text(content)\n",
    "\n",
    "        # å°è¯•è§£æä¸º JSON\n",
    "        parsed = json.loads(cleaned)\n",
    "        return parsed\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"âŒ æ¨¡å‹è¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSONï¼š\")\n",
    "        print(content)\n",
    "        print(\"é”™è¯¯ä¿¡æ¯ï¼š\", e)\n",
    "        return {\"error\": \"Invalid JSON\", \"raw\": content}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯„ä¼°å‡ºé”™ï¼š{e}\")\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97f01d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt_file = f\"{criteria}_{prompt_type}\"\n",
    "prompt = load_prompt_from_file(prompt_file)\n",
    "if not prompt:\n",
    "    print(\"âŒ Prompt åŠ è½½å¤±è´¥ï¼Œé€€å‡ºç¨‹åº\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab24f8f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === è¯»å–å…¨é‡ç”¨æˆ·æ•…äº‹æ•°æ® ===\n",
    "criteria_col = criteria.capitalize()\n",
    "us_df = pd.read_excel(\"data/us/stories.xlsx\", usecols=[\"Issue key\", \"story\", \"ac\", \"bg\", criteria_col])\n",
    "\n",
    "# === æ­¥éª¤ 2ï¼šè¯»å–éœ€è¦è¯„ä¼°çš„ issue_key åˆ—è¡¨ ===\n",
    "atomic_df = pd.read_excel(f\"data/us1/{criteria}.xlsx\", usecols=[\"Issue key\"])\n",
    "\n",
    "# å°†éœ€è¦è¯„ä¼°çš„ Issue key ä¸å…¨é‡æ•°æ®åŒ¹é…\n",
    "df_eval = atomic_df.merge(us_df, on=\"Issue key\", how=\"left\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰ä¸¢å¤±åŒ¹é…é¡¹\n",
    "missing = df_eval[df_eval[\"story\"].isnull()]\n",
    "if not missing.empty:\n",
    "    print(\"âš ï¸ ä»¥ä¸‹ Issue key æœªåœ¨ stories.xlsx ä¸­æ‰¾åˆ°ï¼š\")\n",
    "    print(missing[\"Issue key\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3d5e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for _, row in tqdm(df_eval.iterrows(), total=len(df_eval), desc=\"Evaluating user stories\"):\n",
    "    issue_key = row[\"Issue key\"]\n",
    "    description = row[\"story\"]\n",
    "    expert = row[criteria_col]\n",
    "\n",
    "    if pd.isna(description):\n",
    "        result = {\"error\": \"No description found\"}\n",
    "    else:\n",
    "        result = evaluate_user_story(prompt, description)\n",
    "        time.sleep(1)  # æ¯æ¬¡è°ƒç”¨åæš‚åœ 1 ç§’\n",
    "        agent = -1 # ï¼ˆé»˜è®¤ä¸º -1 è¡¨ç¤ºè§£æå¤±è´¥ï¼‰\n",
    "        if isinstance(result, dict):\n",
    "            v = result.get(f\"violation\")\n",
    "            if v is not None:\n",
    "                agent = 0 if v else 1\n",
    "\n",
    "    results.append({\n",
    "        \"Issue key\": issue_key,\n",
    "        \"Expert\": expert,\n",
    "        \"Agent\": agent,\n",
    "        \"Result\": json.dumps(result, indent=2, ensure_ascii=False)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f4078",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ç”Ÿæˆå½“å‰æ—¶é—´å­—ç¬¦ä¸²ï¼ˆåˆ°åˆ†é’Ÿï¼‰\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "output_file = f\"output/{criteria}-{model_name}-{prompt_type}-{current_time}.xlsx\"\n",
    "pd.DataFrame(results).to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\nâœ… ç»“æœå·²ä¿å­˜è‡³ï¼š{output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fcc36e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === æŒ‡æ ‡è®¡ç®— ===\n",
    "def compute_metrics(df):\n",
    "    df = df[(df[\"Expert\"].isin([0, 1])) & (df[\"Agent\"].isin([0, 1]))]\n",
    "\n",
    "    TP = ((df[\"Expert\"] == 1) & (df[\"Agent\"] == 1)).sum()\n",
    "    TN = ((df[\"Expert\"] == 0) & (df[\"Agent\"] == 0)).sum()\n",
    "    FP = ((df[\"Expert\"] == 0) & (df[\"Agent\"] == 1)).sum()\n",
    "    FN = ((df[\"Expert\"] == 1) & (df[\"Agent\"] == 0)).sum()\n",
    "\n",
    "    total = TP + TN + FP + FN\n",
    "    accuracy = (TP + TN) / total if total else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "    print(\"\\nğŸ“Š è¯„ä¼°æŒ‡æ ‡ï¼š\")\n",
    "    print(f\"TP (True Positive): {TP}\")\n",
    "    print(f\"FP (False Positive): {FP}\")\n",
    "    print(f\"TN (True Negative): {TN}\")\n",
    "    print(f\"FN (False Negative): {FN}\")\n",
    "    print(f\"Accuracy å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
    "    print(f\"Precision ç²¾ç¡®ç‡: {precision:.4f}\")\n",
    "    print(f\"Recall å¬å›ç‡: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# === è®¡ç®—Kappaç³»æ•°\n",
    "def compute_cohen_kappa(results_df):\n",
    "    \"\"\"æ ¹æ® DataFrame ä¸­çš„ Expert ä¸ Agent åˆ—ï¼Œè®¡ç®— Cohen's Kappa ç³»æ•°\"\"\"\n",
    "    if \"Expert\" not in results_df.columns or \"Agent\" not in results_df.columns:\n",
    "        print(\"âŒ ç¼ºå°‘ Expert æˆ– Agent åˆ—ï¼Œæ— æ³•è®¡ç®— Kappa\")\n",
    "        return\n",
    "    \n",
    "    # ç§»é™¤æ— æ•ˆï¼ˆ-1ï¼‰çš„ agent å€¼\n",
    "    filtered = results_df[results_df[\"Agent\"] != -1]\n",
    "\n",
    "    if filtered.empty:\n",
    "        print(\"âš ï¸ æ— æœ‰æ•ˆæ¨¡å‹è¯„ä¼°ç»“æœï¼Œè·³è¿‡ Kappa è®¡ç®—\")\n",
    "        return\n",
    "\n",
    "    y_true = filtered[\"Expert\"].astype(int)\n",
    "    y_pred = filtered[\"Agent\"].astype(int)\n",
    "\n",
    "    kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    print(f\"ğŸ“Š Cohen's Kappa ç³»æ•°ï¼š{kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e69e18e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(results)\n",
    "compute_cohen_kappa(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976ac97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "compute_metrics(df_result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
